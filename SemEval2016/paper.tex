%
% File naaclhlt2016.tex
%

\documentclass[11pt,letterpaper]{article}
\usepackage{naaclhlt2016}
\usepackage{times}
\usepackage{latexsym}
\usepackage{natbib}
\usepackage{url}
\usepackage{xspace}
\usepackage{multirow}

\naaclfinalcopy % Uncomment this line for the final submission
%% **** paper id?
\def\naaclpaperid{510} %  Enter the naacl Paper ID here

% To expand the titlebox for more authors, uncomment
% below and set accordingly.
% \addtolength\titlebox{.5in}    

\newcommand\BibTeX{B{\sc ib}\TeX}

\newcommand{\eqnref}[1]{Equation~\ref{#1}}
\newcommand{\figref}[1]{Figure~\ref{#1}}
\newcommand{\figsref}[2]{Figures~\ref{#1} and \ref{#2}}
\newcommand{\posciteauthor}[1]{\citeauthor{#1}'s}
\newcommand{\poscite}[1]{\citeauthor{#1}'s \citeyearpar{#1}}
\newcommand{\secref}[1]{Section~\ref{#1}}
\newcommand{\secsref}[2]{Sections~\ref{#1} and \ref{#2}}
\newcommand{\sentref}[1]{(\ref{#1})}
\newcommand{\tabref}[1]{Table~\ref{#1}}
\newcommand{\tabsref}[2]{Tables~\ref{#1} and \ref{#2}}

\newcommand{\baselinebin}{\textsc{baseline-bin}\xspace}
\newcommand{\baselinefreq}{\textsc{baseline-freq}\xspace}
\newcommand{\baselinetfidf}{\textsc{baseline-tf-idf}\xspace}
\newcommand{\wordvecsum}{\textsc{word2vec-sum}\xspace}
\newcommand{\wordvecprod}{\textsc{word2vec-prod}\xspace}
\newcommand{\parvec}{\textsc{paragraph-vectors}\xspace}
\newcommand{\skipthoughts}{\textsc{skip-thougts}\xspace}
\newcommand{\skipthoughtsreg}{\textsc{skip-thougts-reg}\xspace}
\newcommand{\average}{\textsc{average}\xspace}
\newcommand{\reg}{\textsc{regression}\xspace}

\renewcommand{\ttdefault}{cmtt}


\title{UNBNLP at SemEval-2016 Task 1: Semantic Textual Similarity: A
  Unified Framework for Semantic Processing and Evaluation}

% Author information can be set in various styles:
% For several authors from the same institution:
\author{Milton King \and Waseem Gharbieh \and SoHyun Park \and Paul Cook
  \\ Faculty of Computer Science, University of New Brunswick\\
Fredericton, NB E3B 5A3, Canada\\
{\tt \{milton.king,waseem.gharbieh,sohyun.park,paul.cook\}@unb.ca}}

 %% Address
 %%  line \\ ... \\ Address line}


% if the names do not fit well on one line use
%         Author 1 \\ {\bf Author 2} \\ ... \\ {\bf Author n} \\
% For authors from different institutions:
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \And  ... \And
%         Author n \\ Address line \\ ... \\ Address line}
% To start a seperate ``row'' of authors use \AND, as in
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \AND
%         Author 2 \\ Address line \\ ... \\ Address line \And
%         Author 3 \\ Address line \\ ... \\ Address line}
% If the title and author information does not fit in the area allocated,
% place \setlength\titlebox{<new height>} right after
% at the top, where <new height> can be something larger than 2.25in
%% \author{Author 1\\
%% 	    XYZ Company\\
%% 	    111 Anywhere Street\\
%% 	    Mytown, NY 10000, USA\\
%% 	    {\tt author1@xyz.org}
%% 	  \And
%% 	Author 2\\
%%   	ABC University\\
%%   	900 Main Street\\
%%   	Ourcity, PQ, Canada A1A 1T2\\
%%   {\tt author2@abc.ca}}

\date{}

\begin{document}

\maketitle

\begin{abstract}
In this paper we consider several approaches to predicting semantic
textual similarity using word embeddings, as well as methods for
forming embeddings for larger units of text. We compare these methods
to several baselines, and find that none of them outperform the
baselines. We then consider both a supervised and unsupervised
approach to combining these methods which achieve modest improvements
over the baselines.
\end{abstract}

\section{Introduction}

Word embeddings \citep{Mikolov+:2013a} have recently led to
improvements in a wide range of tasks in natural language
processing. A number of approaches to forming embeddings for
sentences, paragraphs, and documents have also recently been proposed
\citep[e.g.,][]{Le:Mikolov:2014, Kiros+:2015}. These methods seem
particularly well suited to the task of predicting semantic textual
similarity (STS), and indeed have been shown to work very well on
similar tasks \citep{Kiros+:2015}.

This paper describes the system of UNBNLP at SemEval-2016 Task 1. We
first implement several baseline approaches to STS based on cosine
similarity of count-based vectors representing sentences, with a
variety of approaches to term weighting. We then consider approaches
drawing off of word2vec \citep{Mikolov+:2013a}, paragraph vectors
\citep{Le:Mikolov:2014}, and skip-thoughts \citep{Kiros+:2015}. We
find that none of these approaches improve over any of our baselines.

We then consider combining information from these individual methods
to measuring STS. We consider an unsupervised approach based on the
average of the predicted similarities for a number of these individual
approaches. We further consider a supervised approach in which we
train ridge regression with features corresponding to the similarities
from these individual methods. Each of these methods for combining
information achieves modest improvements over the baselines.



\section{Measuring short text similarity}

We present several methods for measuring STS in
\secref{sec:individualmethods}. We then present approaches to
combining these methods in \secref{sec:combmethods}.


\subsection{Individual methods\label{sec:individualmethods}}

%% This subsection describes the methods for measuring short text
%% similarity that we later combine into more sophisticated similarity
%% measures.

\subsubsection{Baselines}

%% We present three baseline methods. In each method, we represent each
%% sentence in a pair of sentences as a vector, where each dimension
%% corresponds to a word type (i.e., a wordform). 

We present three baseline methods. In all of these baselines, each
sentence in a pair of sentences is represented as a vector, where each
dimension corresponds to a word type (i.e., a word form).

In the first approach, referred to as \baselinebin, the dimensions
hold binary values indicating whether the corresponding type occurs in
the sentence. In the second approach, \baselinefreq, the dimensions
hold the frequency of the corresponding type in the sentence. For the
third approach, \baselinetfidf, each dimension holds the tf-idf weight
for the corresponding type in the sentence. Idf values were calculated
over a 2015 dump of English Wikipedia from 1 September 2015, which was
pre-processed using
wp2txt\footnote{\url{https://github.com/yohasebe/wp2txt}} to remove
markup.

For all baseline methods, the similarity between two sentences is
calculated as the cosine between the vectors representing them.  In
these baseline methods, the documents are tokenized using an approach
suggested by \cite{Speriosu+:2011} --- the text is first split based
on whitespace; for each token, if it contains at least one
alphanumeric character, then all leading and trailing non-alphanumeric
characters are stripped. Stopwords are removed based on a stopword
list,\footnote{\url{http://www.lextek.com/manuals/onix/stopwords1.html}}
and case folding is applied.



%% Milton: What list of stopwords was used? A pre made stopword list from http://www.lextek.com/manuals/onix/stopwords1.html. 429 words in it

%% Milton: was case folding used? Yes case folding was used


\subsubsection{Word2vec}

We considered two methods based on word embeddings from word2vec
\citep{Mikolov+:2013a}. For each sentence, we formed a vector
corresponding to the element-wise summation, and product, of the word
embeddings for each token in that sentence. We then measure the
similarity of two sentences as the cosine between their vector
representations. We refer to these methods as \wordvecsum and
\wordvecprod, respectively.

For this method, we used pre-trained word2vec vectors provided by
Google.\footnote{\url{https://code.google.com/archive/p/word2vec/}}
These vectors have 300 dimensions, and were trained on a corpus of
documents from Google News that contained approximately 100 billion
tokens.

For this method, sentences were tokenized by splitting on whitespace,
and then removing non-alphanumeric characters. The text was also
case-folded.


\subsubsection{Paragraph vectors}

Paragraph Vectors \citep{Le:Mikolov:2014} is an extension of word2vec
\citep{Mikolov+:2013a} to text of arbitrary length. In our
implementation, we used the Distributed Memory Model of Paragraph
Vectors (PV-DM) to represent each sentence as a vector. The similarity
between two sentences was then computed as the cosine of their vector
representations. We refer to this approach as \parvec.

%% This is done by training the model first and then using the trained
%% parameters to infer a fixed-length vector representation of the
%% sentence. The aim of the model is to be able to predict the next word
%% in a paragraph given a sequence of tokens. During training, a fixed
%% length window of tokens is sampled from a random paragraph and input
%% into the model. The resulting word vectors can be either summed or
%% concatenated with the paragraph vector.  Stochastic gradient descent
%% is then used to update the parameters of the model. During inference,
%% all the word vectors are kept fixed and gradient descent is used to
%% update the parameters of the new paragraph vector. The resulting
%% paragraph vector is then used for finding the similarity between two
%% sentences.

The gensim\footnote{\url{https://radimrehurek.com/gensim/}}
implementation of the PV-DM model was trained on a roughly 540 million
token sample of English Wikipedia. To tokenize the Wikipedia corpus,
the text was first split based on whitespace; then, all
non-alphanumeric characters, except for \emph{+}, \emph{-}, \emph{\$}
and \emph{\%}, were removed. The remaining tokens wore
case-folded. Tokens that did not have a Unicode encoding, or that
occurred less than 5 times in the corpus were removed. During
training, every paragraph in the corpus was treated as a separate
paragraph in the model.\footnote{This model can be applied to various
  units of text, e.g., sentence, paragraph, document.}
%% and the resulting paragraph vector was concatenated with the word
%% vectors.

%% In preliminary experiments we noted that increasing the dimensionality
%% of the paragraph representations generally resulted in higher
%% correlation scores on the SemEval development data. 

The dimensionality of the word and paragraph representations was set
to 400.  A window size of 8 was used. The negative sampling parameter
was set to 20. The subsampling parameter was set to ${10}^{-5}$.
After training the model, the vector representing each sentence was
inferred.


\subsubsection{Skip-thoughts\label{sec:skipthoughts}}

\begin{table*}
\small
% Sets width between columns
 \setlength{\tabcolsep}{3pt}
\begin{center}
\begin{tabular}{lcccccc}
Method & Answer--answer & Headlines & Plagiarism & Post-editing & Question--question & All \\
\hline
\baselinebin & 0.50937 & 0.70636 & \textbf{0.80108} & \phantom{-}0.76370 & \phantom{-}0.61827 & 0.67881\\ 
\baselinefreq &0.44204 & \textbf{0.72754} &0.79604 & \phantom{-}0.79483 & \phantom{-}\textbf{0.65749} & 0.68122\\
\baselinetfidf (Run 1)  & 0.45928 & 0.66593 & 0.75778 & \phantom{-}0.77204 & \phantom{-}0.61710 & 0.65271\\ 
\wordvecprod  & 0.39310 & 0.60667 & 0.71528 & \phantom{-}0.21306 & \phantom{-}0.10847 & 0.41322\\ 
\wordvecsum & 0.13521 & 0.14328 & 0.23290 & -0.02673 & \phantom{-}0.25153 & 0.14303\\
%% Word2vec (Regression) & 0.41710 & 0.65769 & 0.66245 & \phantom{-}0.71492 & \phantom{-}0.46182 &\\
\parvec  & 0.41123 & 0.69169 & 0.60488 & \phantom{-}0.75547 & -0.02245 & 0.50206\\ 
\skipthoughts  & 0.27148 & 0.23199 & 0.49643 & \phantom{-}0.48636 & \phantom{-}0.17749 & 0.33446\\ 
\skipthoughtsreg &0.28626 & 0.51019 & 0.66708 & 0.69947 & 0.40459 & 0.51299\\ 
\hline
\average (Run 2) & \textbf{0.58520} & 0.69006 & 0.78923 & \phantom{-}\textbf{0.82540} & \phantom{-}0.58605 & 0.69635 \\ 
\reg (Run 3) & 0.55254 & 0.71353 & 0.79769 & \phantom{-}0.81291 & \phantom{-}0.62037 & \textbf{0.69940}\\ 
\end{tabular}
\caption{Pearson correlation for each method, on each dataset, as well
  as the weighted average correlation over all datasets (``All''). The
  best method on each dataset, and over all datasets, is shown in
  boldface.\label{tbl:results}}
\end{center}
\end{table*}



Skip-thoughts \citep{Kiros+:2015} can be viewed as an extension of the
word2vec skipgram model for obtaining vector representations of
sentences. Skip-thoughts is primarily an encoder--decoder model
composed of gated recurrent units (GRUs). A GRU \citep{Cho2014} is a
recurrent neural network used for sequence modeling
\citep{Chung2014}. It is similar to long short-term memory
\citep{hochreiter1997long}, but with a simplified gating architecture
that does not include separate internal memory cells. The encoder
receives a sequence of tokens from a sentence, and the decoder
attempts to predict the sentence before the input sentence, and the
sentence after it. Once the model has been trained, the vector
representation of a sentence can be extracted from the learned encoder
by inputting the sequence of tokens that makes up the sentence.

We used the pre-trained combine-skip model provided by
\cite{Kiros+:2015} to build the vector representation of
sentences. This produces a 4800 dimensional vector for each sentence
by concatenating the vector representations from the uni-skip model
and the bi-skip model. The uni-skip model is a unidirectional encoder
that encodes the input tokens of a sentence in their original order,
and outputs a 2400 dimensional vector. The bi-skip model is a
bidirectional model that encodes the input tokens of a sentence in
their original order, and in their reversed order, outputting a 1200
dimensional vector for each direction. The similarity between two
sentences is then computed by taking the cosine similarity of their
vector representations. This method is referred to as \skipthoughts.

We further considered a supervised approach based on skip-thought
vectors. We again formed a vector representing each sentence using the
pre-trained model provided by \citeauthor{Kiros+:2015}~ Then,
following \citeauthor{Kiros+:2015}, we represented each pair of
sentences as a vector consisting of the concatenation of the
componentwise product, and absolute difference, of the vectors
representing the sentences. That is, if $\vec{u}$ and $\vec{v}$ are
the $d$-dimensional skip-thought vectors representing two sentences,
we represent this sentence pair as a $2d$-dimensional vector
consisting of the concatenation of $\vec{u} \circ \vec{v}$ and
$|\vec{u} - \vec{v}|$. We trained ridge regression using gold-standard
STS data from 2012, 2013 and 2015, and then used this model to predict
similarity for the test sentence pairs. We refer to this model as
\skipthoughtsreg. We implemented this model after submitting our
official runs.


\subsection{Method combinations\label{sec:combmethods}}

We used two different methods --- one unsupervised, and one supervised
--- to combine the individual methods in an effort to develop a
stronger system.

For the unsupervised method, \average, we computed the average of
\baselinebin, \baselinetfidf, \wordvecprod, \parvec, and
\skipthoughts. We did not consider \baselinefreq here because it is
quite similar to \baselinebin, which performed better on development
data.

For the supervised approach to combining individual methods, we
trained ridge regression over the similarities produced by the
following methods: \baselinebin, \baselinetfidf, \parvec, and
\skipthoughts.
%% Milton: Why didn't we use word2vec here? I came across some problems with running the regression model with it. 
The ridge regression was trained using the gold standard data provided
for STS tasks in 2012, 2013, and 2015; this model was then used to
predict similarities for sentence pairs in the test data. We refer to
this method as \reg.

\section{Results}



Results for each method, on each dataset, are shown in
\tabref{tbl:results}. We first consider the baseline approaches. On
development data from previous STS tasks, \baselinetfidf gave higher
correlations than baselines based on word presence (\baselinebin) or
word frequency (\baselinefreq). Moreover, this was a challenging
baseline to beat, and was among the best methods we considered on the
development data. It was therefore submitted as one of our official
runs. However, on the test data, \baselinetfidf had the lowest average
correlation of the three baseline approaches considered.

In terms of the methods based on word2vec, representing a sentence as
the componentwise product of the vectors for the words in that
sentence (\wordvecprod) performed much better than the approach based
on vector addition (\wordvecsum). \parvec outperformed both word2vec
approaches. However, none of these word embedding-based methods
performed as well as any of the baselines.

Naively measuring similarity as the cosine between skip-thought
vectors for the sentences in a pair (\skipthoughts) led to relatively
poor performance. Training ridge regression based on features derived
from skip-thought vectors (\skipthoughtsreg, described in
\secref{sec:skipthoughts}) led to substantial improvements, although
again this approach did not beat any of the baselines.

\average and \reg~--- both submitted as official runs --- combine
several of the individual methods together, and both achieve
correlations that are, overall, better than those of any of the
baselines. However, the improvements are relatively modest. Although
there is some variation across the datasets, \average and \reg perform
very similarly overall. \average, however, has an advantage, in that
it is an unsupervised approach.


%% Average combines these:
%% \baselinebin, \baselinetfidf, \wordvecprod, \parvec, and \skipthoughts

%% Regression combines these:
%% \baselinebin, \baselinetfidf, \parvec, and \skipthoughts.



%% Official results from SemEval organizers:

%% Run: STS2016.OUTPUTUNBNLP.tf-idf
%%    =============================================
%%    ALL                 0.65271
%%    answer-answer       0.45928
%%    headlines           0.66593
%%    plagiarism          0.75778
%%    postediting         0.77204
%%    question-question   0.61710

%%    Run: UNBNLP.Average
%%    =============================================
%%    ALL                 0.69635
%%    answer-answer       0.58520
%%    headlines           0.69006
%%    plagiarism          0.78923
%%    postediting         0.82540
%%    question-question   0.58605

%%    Run: UNBNLP.Regression
%%    =============================================
%%    ALL                 0.69940
%%    answer-answer       0.55254
%%    headlines           0.71353
%%    plagiarism          0.79769
%%    postediting         0.81291
%%    question-question   0.62037



\section{Conclusions}

In this paper we first considered several baseline approaches to
STS. We then considered approaches based on word2vec, paragraph
vectors, and skip-thoughts. We found that none of these approaches
improved over any of the baselines. We further considered combining
these approaches via averaging, and a supervised approach based on
regression, and achieved modest improvements over the baselines.

\section*{Acknowledgments}
This work is financially supported by the Natural Sciences and
Engineering Research Council of Canada, the New Brunswick Innovation
Foundation, and the University of New Brunswick.

\bibliography{../bibtex/big}
\bibliographystyle{acl_natbib}


\end{document}
