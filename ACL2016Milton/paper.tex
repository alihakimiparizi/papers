%
% File ACL2016.tex
%

\documentclass[11pt]{article}
\usepackage{eacl2017}
\usepackage{times}
\usepackage{latexsym}
\usepackage{natbib}
\usepackage{url}
\usepackage{xspace}
\usepackage{multirow}


%\aclfinalcopy % Uncomment this line for the final submission
%\def\aclpaperid{***} %  Enter the acl Paper ID here

% To expand the titlebox for more authors, uncomment
% below and set accordingly.
% \addtolength\titlebox{.5in}    

%% ***** Decide where to introduce skip thoughts. Could just be in the
%% results section as another method that we compare against. *****

%% ***** Blurb describing word2vec *****

\newcommand\BibTeX{B{\sc ib}\TeX}

\newcommand\original{\textsc{Original}\xspace}
\newcommand\twitter{\textsc{Twitter}\xspace}

\newcommand{\eqnref}[1]{Equation~\ref{#1}}
\newcommand{\figref}[1]{Figure~\ref{#1}}
\newcommand{\figsref}[2]{Figures~\ref{#1} and \ref{#2}}
\newcommand{\posciteauthor}[1]{\citeauthor{#1}'s}
\newcommand{\poscite}[1]{\citeauthor{#1}'s \citeyearpar{#1}}
\newcommand{\secref}[1]{Section~\ref{#1}}
\newcommand{\secsref}[2]{Sections~\ref{#1} and \ref{#2}}
\newcommand{\sentref}[1]{(\ref{#1})}
\newcommand{\tabref}[1]{Table~\ref{#1}}
\newcommand{\tabsref}[2]{Tables~\ref{#1} and \ref{#2}}

\title{Supervised and unsupervised approaches to measuring usage similarity}

% Author information can be set in various styles:
% For several authors from the same institution:
% \author{Author 1 \and ... \and Author n \\
%         Address line \\ ... \\ Address line}
% if the names do not fit well on one line use
%         Author 1 \\ {\bf Author 2} \\ ... \\ {\bf Author n} \\
% For authors from different institutions:
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \And  ... \And
%         Author n \\ Address line \\ ... \\ Address line}
% To start a seperate ``row'' of authors use \AND, as in
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \AND
%         Author 2 \\ Address line \\ ... \\ Address line \And
%         Author 3 \\ Address line \\ ... \\ Address line}
% If the title and author information does not fit in the area allocated,
% place \setlength\titlebox{<new height>} right after
% at the top, where <new height> can be something larger than 2.25in
\author{}

\date{}

\begin{document}

\maketitle

\begin{abstract}

Usage similarity is an approach to determining word meaning in context
that does not rely on a sense inventory. Instead, pairs of usages of a
target lemma are rated on a scale. In this paper we present an
unsupervised approach to measuring usage similarity based on word
embeddings that achieves state-of-the-art performance on two usage
similarity datasets. We further consider a supervised approach to
usage similarity, and find that although it outperforms the
unsupervised approach, it is unable to generalize to lemmas that are
unseen in the training data.
\end{abstract}

\section{Introduction}

Word senses are not discrete. In many cases, for a given instance of a
word, multiple senses from a sense inventory are applicable, and to
varying degrees \citep{Erk2009b}. That a clear line cannot be drawn
between the various senses of a word has been observed as far back as
\cite{Johnson1755}. Some have gone so far as to doubt the existence of
word senses \citep{Kilgarriff1997}.

%% Quote from Johnson's preface:
%% The shades of meaning sometimes pass imperceptibly into each other;

Sense inventories also suffer from a lack of coverage. New words
regularly come into usage, as do new senses for established
words. Furthermore, domain-specific senses are often not included in
general-purpose sense inventories.
%% Citations to back these claims up???
This issue of coverage is particularly relevant for social media text,
which contains a higher rate of out-of-vocabulary words than
more-conventional text types \citep{Baldwin+:2013}.

These issues pose problems for natural language processing tasks such
as word sense disambiguation and induction, which rely on, and seek to
induce, respectively, sense inventories. In response to this,
alternative approaches to word meaning have been proposed that do not
rely on sense inventories \citep{Erk2009b}. In particular,
\citeauthor{Erk2009b} introduce the task of usage similarity (USim),
in which two usages of a given target word are rated for their
similarity, without reference to a sense inventory.

In this paper we propose an unsupervised approach to USim based on
word embeddings \citep{Mikolov+:2013b} that achieves state-of-the-art
results over two USim datasets, based on Twitter text and
more-conventional texts, respectively.  We then consider a supervised
approach to usage similarity. Although this method outperforms the
unsupervised approaches, it performs very poorly on lemmas that are
unseen in the training data.

\section{Related work\label{sec:rw}}

Word sense disambiguation (WSD) is the task of selecting the
most-appropriate sense --- from a sense inventory --- for a token
instance of a word in context \citep{Navigli:2009}. This task
specifically requires a sense inventory, and has traditionally assumed
that each instance of a word can be assigned one sense.  Word sense
induction (WSI), on the other hand, is the task of clustering the
token instances of a given word by sense, in order to induce word
senses. Although WSI does not require a sense inventory, this task
typically assumes that each usage can be assigned to one cluster
(i.e., induced sense). Moreover, WSI evaluation has often been based
on a WSD task, where induced senses are mapped to gold-standard senses
\citep{Agirre:Soroa:2007,Manandhar+:2010}. Recent WSI systems and
evaluations have, however, considered graded senses and multi-sense
applicability \citep{Jurgens:Klapaftis:2013}.

\cite{Erk2009b} showed that multiple senses are often applicable to a
given instance of a word, and that this issue cannot be addressed
simply by choosing a coarser-grained sense inventory.  As part of
their work they carried out an annotation task on ``usage similarity''
(USim), in which the similarity of the meanings of two usages of a
given word are rated on a five-point scale.

\cite{LuiBaldwin2012} proposed the first computational approach to
USim. They considered approaches based on topic modelling
\citep{Blei2003}, under a wide range of parameter settings, and found
that a single topic model for all target lemmas (as opposed to one
topic model per target lemma) 
%% based on the full-document context in which each target token
%% occurred,
performed best on the dataset of \cite{Erk2009b}. \cite{Gella+:2013}
considered USim on Twitter text, noting that this model of word
meaning seems particularly well-suited to this text type because of
the prevalence of out-of-vocabulary words. \citeauthor{Gella+:2013}
also considered topic modelling-based approaches, achieving their best
results using one topic model per target word, and a document
expansion strategy based on medium frequency hashtags to combat the
data sparsity of tweets due to their relatively short length. The
methods of \citeauthor{LuiBaldwin2012} and \citeauthor{Gella+:2013}
are unsupervised; they do not rely on any gold standard USim
annotations.
%% Reinforce that these would need to be set somehow... and that it's
%% not entirely clear how to do that?

%% ***** Revise the story. Simple method that achieves state of the
%% art. Consider a supervised approach that does even better but doesn't
%% generalize to new lemmas. Somehow work skipthoughts in *****

Our proposed method achieves state of the art performance on
\poscite{Erk2009b} original USim dataset, and \poscite{Gella+:2013}
Twitter USim dataset.


Contex2vec is a bidirectional long short term memory neural network that embeds the context of a target token by feeding the tokens at the beginning of the sentence into the forward neural network one at a time and preserving the order, until the target token is reached. likewise, the reversed order of tokens from the end of the sentence are used as input to the backward neural network. The concatenation of the states of these two networks is the embedding of the given token. 

\cite{Kiros+:2015} recently proposed skip-thoughts, a sentence encoder
that can be viewed as a sentence-level version of skipgram, i.e.,
during training the model, the encoding of a sentence is used to
predict surrounding sentences.

\cite{Mikolov+:2013a} proposed a model that embeds a document of a non-predetermined lengths into a vector of a predetermined length. 

%\cite{pennington2014glove} proposed Global Vectors (GloVe), which 
%Explain GloVeif need be

%% over the
%% method of \cite{LuiBaldwin2012} on the original USim dataset
%% \cite{Erk2009b} and that of \cite{Gella+:2013} on their Twitter USim
%% dataset. 




%% In contrast to the approaches of \citeauthor{LuiBaldwin2012}
%% and \citeauthor{Gella+:2013}, our method is relatively robust to
%% parameter settings.


%% ***** Need to talk about short text similarity tasks, Skip thoughts
%% *****


\section{A continuous vector space model for usage similarity\label{sec:model}}

%% Represent the meaning of a word by its context. 

In this paper we consider both unsupervised and supervised approaches
for USim.

\subsection{Usage representation}
%%DIscuss the different methods of representing a usage for the different models
We used four different embeddings models to obtain representations of a given usage. Using the word2vec model, we represent a token instance of the target word in a sentence as the
sum of the word embeddings for the other words occurring in the
sentence, excluding stopwords. For the context2vec, skip-thoughts, and doc2vec models, we fed the sentence containing the target word as input to each of these embedding models, where each will create an embedding of the usage. 

\subsection{Unsupervised approach}


%% the types corresponding to the other tokens occurring in that
%% sentence,
In the unsupervised setup, we measure the similarity between two usages of the target word
as the cosine between their vectors that are produced by one of the previously discussed methods. 

%% This gives an unsupervised approach to computing USim.

%% ***** Could clarify that this is unsupervised in that it doesn't
%% rely on labeled data about usim *****

\subsection{Supervised approach}

We also consider a supervised approach to USim. For a given pair of
token instances of a target word, $t_1$ and $t_2$, we first form
vectors $v_1$ and $v_2$ representing each of the two instances of the
target as described in the previous subsection. Following
\cite{Tai+:2015} we compute the componentwise product, and absolute
difference, of $v_1$ and $v_2$, and concatenate them. This gives a
vector of length $2d$ --- where $d$ is the dimensionality of the word
embeddings used --- representing each pair of instances. We then train
ridge regression to learn a model to predict the similarity of unseen
usage pairs. The training data consists of usage pairs of many lemmas;
i.e., we do not train a separate  model per lemma.

%% PC: Are there any parameters we had to set for ridge regression?
%% Normalize = True. So that the training data is normalized before
%% the regression is applied

\section{Materials and methods\label{sec:mandm}}

%% We trained four sets of word embeddings using Wor2Vec
%% \citep{Mikolov+:2013b} on two different corpora. Two sets were trained on
%% a Twitter Dump, while the other sets were trained on a Wiki dump. One
%% set of each of the corpus dependent sets were trained using the
%% Skipgram model and the other sets were trained using the CBOW
%% model. Each set consisted of models that were trained using a variety
%% of parameters for dimensions of the embeddings and the size of the
%% training window.  We then test each individual model on two different
%% usage similarity tasks. The two datasets tested that were tested on
%% are the ones that were used in \cite{Gella+:2013} and
%% \cite{LuiBaldwin2012}. These are further described in section 4.1. We
%% tokenized each dataset before applying running the USim test. The
%% dataset that was used in \cite{LuiBaldwin2012} was tokenized using a
%% basic tokenizer that we developed where all leading and trailing
%% non-alphanumeric characters are dropped and the dataset that was used
%% in \cite{Gella+:2013} was tokenized using a tokenizer, which made to
%% handle Twitter text which is explained in 4.3. We approached this task
%% by comparing a vector representation of each sentence. The vector
%% representation of a sentence is achieved through summing over all word
%% embeddings within that sentence. The sentences were first tokenized
%% then we iterated over the sentence and if the word is not in the stop
%% word list, then the embedding of the word is grabbed from the trained
%% model and added to the ongoing sum of word embeddings using component
%% wise addition. Once both sentences within a sentence pair have a
%% vector representation, we compute the cosine distance between them as
%% measurement of similarity. In preliminary experiments, we tested with
%% different parameters for the size of the window around the word whose
%% usage similarity is being calculated. This means we only summed the
%% embeddings of the words within this window from that word. In
%% preliminary experiments, we discovered that a window, that viewed all
%% words in the sentence, shown to produce better results in terms of
%% correlation with the gold standard.

\subsection{USim Datasets \label{sec:datasets}}

We consider two USim datasets in this work, to evaluate our methods on
two different text types: the original USim dataset (\original) of
\cite{Erk2009b} and the Twitter USim dataset (\twitter) of
\cite{Gella+:2013}. Both USim datasets contain pairs of sentences;
each sentence in each pair includes a usage of a particular target
lemma. Each sentence pair is rated on a scale of 1--5 for how similar
in meaning the usages of the target words are in the two sentences.

\original consists of sentences from \cite{McCarthy:Navigli:2007},
which were drawn from a web corpus \citep{Sharoff2006b}. This dataset
contains 34 lemmas, including nouns, verbs, adjectives, and
adverbs. Each lemma is the target word in 10 sentences. For each
lemma, sentence pairs are formed based on all pairwise comparisons,
giving 45 sentence pairs per lemma. Annotations were provided by three
native English speakers, with the average taken as the final gold
standard similarity. In a small number of cases the annotators were
unable to judge similarity. \citeauthor{Erk2009b} removed these
sentence pairs from the dataset, resulting in a total of 1512 sentence
pairs.

\twitter contains sentence pairs for ten nouns from \original. In this
case the ``sentences'' are in fact tweets. 55 sentence pairs are
provided for each noun. Unlike \original, the sentence pairs are not
formed on the basis of all pairwise comparisons amongst a smaller set
of sentences. This dataset was annotated via Amazon Mechanical
Turk,\footnote{\url{https://www.mturk.com/}} and carefully cleaned to
remove outlier annotations.

%% Omitted detail that they applied lexical normalization using Bo's
%% dictionary to the dataset



%% \original was tokenized by splitting on whitespace, and then stripping
%% all leading and trailing non-alphanumeric characters from tokens
%% containing at least one alphanumeric character. \twitter was tokenized
%% using a tokenizer developed for tweets, based on
%% \cite{OConnor+:2010}. This tokenizer preserves phenomena that are
%% particularly common in tweets --- such as emoticons, hashtags, and
%% urls --- as tokens.



\subsection{Evaluation\label{sec:evaluation}}
Following \cite{LuiBaldwin2012} and \cite{Gella+:2013} we evaluated
our systems by calculating Spearman's rank correlation coefficient
between the gold standard similarities and the predicted
similarities. This enables direct comparison of our results with those
reported in these previous studies.


%% We considered two cross-validation methodologies to evaluate our
%% supervised approaches.

%% For experiments involving our supervised approaches, we considered two
%% evaluation methodologies based on cross-validation.

We evaluated our supervised approaches using two cross-validation
methodologies. In the first case we applied 10-fold cross-validation,
randomly partitioning all sentence pairs for all lemmas in a given
dataset into sub-samples. Using this approach, the test data for a
given fold consists of sentence pairs for target lemmas that were seen
in the training data. To determine how well our methods generalize to
unseen lemmas, we considered a second approach to cross-validation. In
this case we partitioned the sentence pairs in a given dataset by
lemma. For a given fold, the test data consisted of all sentence pairs
for one lemma, and the training data consisted of sentence pairs for
all other lemmas.

\subsection{Corpora\label{sec:corpora}}

% Describe corpora (Twitter and Wikipedia) and their preprocessing

The word embeddings were trained on two corpora: (1) a sample of
English tweets collected from the Twitter Streaming
APIs\footnote{\url{https://dev.twitter.com/}} from November 2014 to
March 2015, consisting of roughly 1.3 billion tokens; (2) a dump of
English Wikipedia from 1 September 2015, containing roughly 2.6
billion tokens. The Twitter corpus was tokenized using a tokenizer
developed for tweets, based on \cite{OConnor+:2010}. This tokenizer
preserves phenomena that are particularly common in tweets --- such as
emoticons, hashtags, and URLs --- as tokens. The Wikipedia dump was
first processed using
WP2TXT\footnote{\url{https://github.com/yohasebe/wp2txt}} to remove
markup. This corpus was then tokenized using the Stanford PTBTokenizer
\citep{manning-EtAl:2014:P14-5}.


\subsection{Word2vec settings\label{sec:w2vparams}}
We trained the word embeddings using word2vec's skipgram model for a
variety of settings of window size ($W$=2,5,8) and number of
dimensions ($D$=50,100,300) on both the Wikipedia and Twitter corpora
described in the previous subsection.\footnote{In preliminary
  experiments the alternative word2vec CBOW model achieved
  substantially lower correlations than skipgram, and so CBOW was not
  considered further. When using word2vec models in the Twitter case, the target word is included but not in the Orig case.}

\section{Experimental results\label{sec:results}}





%% ***** Fix labelling of significant correlations *****

\begin{table*}
\begin{center}
\small
\begin{tabular}{cc|ccc|ccc}
& & \multicolumn{3}{c|}{\textsc{Original}} &
  \multicolumn{3}{c}{\textsc{Twitter}}\\ 

 &  & \multirow{2}{*}{Unsupervised}  & \multicolumn{2}{c|}{Supervised} & \multirow{2}{*}{Unsupervised} & \multicolumn{2}{c}{Supervised}  \\ 
$W$ & $D$ & & All & Lemma & & All & Lemma\\
\hline
%%New values with the proper seed
1 & 50  & 0.236* & 0.284* & 0 & 0.202* & 0.267 & 0.154 \\   
1 & 100 & 0.256* & 0.307* & 0. & 0.239* & 0.287 & 0.207 \\   
1 & 300 & 0.266* & 0.378* & 0. & 0.237* & 0.186 & 0.032 \\  
2 & 50  & 0.251* & 0.310* & 0 & 0.231* & 0.301 & 0.128 \\   
2 & 100 & 0.267* & 0.373* & 0 & 0.276* & 0.261\phantom{*} & 0.155\\  
2 & 300 & 0.275* & 0 .421* & 0 & 0.266* & 0.290\phantom{*} & 0.182 \\  
\hline
5 & 50  & 0.262* & 0.330* & 0 & 0.272* & 0.331 & 0.206 \\   
5 & 100 & 0.273* & 0.352* & 0 & 0.295* & 0.278 & 0.160 \\  
5 & 300 & 0 & 0 & 0 & 0.295* & 0.347* & 0.225 \\  
\hline
8 & 50  & \textbf{0.286*} & 0.355* & 0 & 0.282* &0.311\phantom{*} & \textbf{0.254} \\   
8 & 100 & 0.273* & 0.376* & 0 & 0.298* & 0.342* & 0.236 \\  
8 & 300 & 0.281* & \textbf{0.434*} & 0 & \textbf{0.310*} & \textbf{0.379*} & 0.138 \\    
%% 10 & 50 & 0.279* & 0  & \textbf{0} & 0. & 0 & 0 \\  
%% 10 & 100& 0  &  0   & 0 & 0. & 0 & 0 \\ 
%% 10 & 300& 0.281*   &  \textbf{0}   & 0 & ???   & &  ??? \\
\hline
\end{tabular}
\caption{Spearman's $\rho$ on \original and \twitter using the
  unsupervised and supervised methods with Ridge Regression, with cross-validation folds
  based on random sampling across all lemmas (All), or holding out
  individual lemmas (Lemma), for word embeddings trained using a
  variety of settings for window size ($W$) and number of dimensions
  ($D$), and skip-thought vectors. The best $\rho$ for each method is
  shown in boldface. Significant correlations ($p<0.05$) are indicated
  with *. Context2vec trained onUkWaC.
\label{tbl:results}}

\end{center}
\end{table*}


\begin{table*}
\begin{center}
\small
\begin{tabular}{cc|ccccc|c}

Dataset& &Word2vec(Wiki/Twitter) & Word2vec(UkWaC) & Skip-thought & Context2vec & GLoVe & Previous state-of-the-art \\
\hline

& Unsupervised  & 0.281* &0.276* & 0.177*& \textbf{0.302*}& 0.218* & \multirow{7}{*}{0.202}\\
Original &All (sup)  & 0.434*& \textbf{0.450*} & 0.433* & 0.405* & 0.406*\\
&Abs Diff All (sup)  & 0.370* &0.429* & \textbf{0.462*} & 0.406* & 0.317*\\
&Prod All (sup)  & \textbf{0.386*} & 0.385* & \textbf{0.386*} & 0.352* & 0.356*\\
&Lemma (sup)  & \textbf{0.209} &0.237& 0.122& 0.198 & 0.228 \\
&Abs Diff Lemma (sup)  &  \textbf{0.186} & 0.214 &0.140 & 0.094 &0.154 \\
&Prod Lemma (sup)  & 0.213 & 0.237 & 0.112 & \textbf{0.214} & 0.253\\
\hline

& Unsupervised  &0.300* & 0& 0.095*&  0 & 0.122* &\multirow{7}{*}{0.290}  \\
Twitter &All (sup)  & 0.379*& 0& 0.364*& 0& 0.319 & \\
&Abs Diff All (sup)  & 0.303 & 0 & \textbf{0.292*} & 0 & 0 \\
&Prod All (sup)  & 0.378* & 0 & \textbf{0.355*} & 0 & \\
&Lemma (sup) & 0.203 & 0& 0.079 & 0& & \\
&Abs Diff Lemma (sup)  & 0.172 & 0 & \textbf{0.032} & 0 & \\
&Prod Lemma (sup)  & 0.213 & 0 & 0.074 & \textbf{0} & \\
\hline
\end{tabular}
\caption{Spearman's $\rho$ on \original and \twitter using the
  unsupervised and supervised methods with Ridge Regression, with cross-validation folds
  based on random sampling across all lemmas (All), or holding out
  individual lemmas (Lemma), for word embeddings trained using a
  variety of settings for window size ($W$) and number of dimensions
  ($D$), and skip-thought vectors. The best $\rho$ for each method is
  shown in boldface. Significant correlations ($p<0.05$) are indicated
  with *. Word2vec models trained on Wiki with a window size of 8 and 300 dimensions. Context2vec trained on UkWaC. GLoVE has a dimension of 300 for orig and 200 for Twitter.
%%Target lemma is not included in the Word2Vec setup for above numbers 
\label{tbl:results}}
\end{center}
\end{table*}



Results for the supervised and unsupervised methods are shown in
\tabref{tbl:results}. We first consider results for the unsupervised
approach. On both \original and \twitter, for a given window size
($W$), as the number of dimensions ($D$) is increased, $\rho$
increases, with the exception of $W$=8 for \original, where $D$=50
achieves the highest correlation. Similarly, for a given number of
dimensions, larger window sizes achieve higher $\rho$.

%% ***** Say something about this: more information carried by higher
%% dimensionalities and window sizes is good... Can we compare this to
%% any other tasks??? *****

The best correlations reported by \cite{LuiBaldwin2012} on \original,
and \cite{Gella+:2013} on \twitter, were 0.202 and 0.29,
respectively. The best parameter settings for our unsupervised
approach achieve substantially higher correlations, 0.285 and 0.364,
on \original and \twitter, respectively. \citeauthor{LuiBaldwin2012}
and \citeauthor{Gella+:2013} both report report drastic variation in
performance for different settings of the number of topics in their
models. The performance of our unsupervised approach also exhibits
some variation with respect to parameter settings; however, any of the
parameter settings considered, with the exception of very short 50
dimensional vectors for a window size of 2 or 5 for \twitter, give
higher correlations than the previous best reported.


%% Comment on skip thoughts in the unsupervised setup when those
%% results come in? Or possibly just discuss skip thoughts at the
%% end?


Turning to the supervised approach, we first consider results for
cross-validation based on randomly partitioning all sentence pairs in
a dataset (``All'' in \tabref{tbl:results}). In all cases, this
supervised approach is an improvement over the unsupervised approach
for the same parameter settings. The best correlations on \original
and \twitter are 0.462 and 0.442, respectively. This suggests that if
labeled training data is available, supervised approaches can give
substantial improvements over unsupervised approaches to predicting
USim.\footnote{These results on \original must be interpreted
  cautiously, however. The same sentences, albeit in different
  sentence pairs, occur in both the training and testing data for a
  given fold. This issue does not affect \twitter.}
%% ***** Comment on the duplication I noted in the twitter dataset??? *****
However, this experimental setup does not demonstrate the extent to
which such a supervised approach to USim is able to generalize to
lemmas that are not seen during training.

We now consider the supervised approach for cross-validation using
lemma-based partitioning. In these experiments, the test data consists
of sentence pairs for a target lemma that was not seen as a target
lemma in the sentence pairs in the training data. In this case, the
correlation is always lower than the corresponding settings for the
unsupervised approach (and the supervised approach in the
previously-discussed cross-validation setup). This demonstrates that
the current supervised approach generalizes poorly to new
lemmas. Although this is a negative result, it indicates an important
direction for future work --- identifying strategies to training
supervised approaches to predicting USim that generalize to unseen
lemmas.

 As part of their evaluation,
\citeauthor{Kiros+:2015} showed that skip-thoughts out-performs
previous approaches to sentence-level relatedness on the SICK dataset
\citep{Marelli+:2014}. Although our goal is to determine the meaning
of a word in context, the meaning of a sentence could be a useful
proxy for this. \citeauthor{Kiros+:2015} released a pre-trained model,
based on a corpus of books, which we apply to infer the skip-thought
vectors for each sentence in our datasets.\footnote{Inference requires
  only a single sentence. The model is therefore able to infer
  skip-thought vectors for sentences out-of-context, and for tweets
  that typically do not form part of a larger discourse.} We then
consider these vectors in the unsupervised approach, and the
supervised approach for both cross-validation settings. Results are
shown in the bottom row of \tabref{tbl:results}.  Although this
approach performed poorly in the unsupervised setting, it achieved the
highest correlation in the ``All'' supervised setting for
\original. The comparatively low correlation here on \twitter could be
due to the differences in text type (i.e., books vs. tweets).  Based
on this finding, in future work, we intend to evaluate alternative
approaches to forming representations of sentences
\citep[e.g.,][]{Le:Mikolov:2014} for the task of predicting usage
similarity.


%% A number of approaches to representing the meaning of sentences have
%% recently been proposed.

%% ***** Comment on considering these *****
%% Although our goal is to determine the meaning of a word in context,
%% the meaning of a sentence could be a useful proxy for this; 
%% ***** What goes here will depend on whether we keep the skip-thought
%% stuff in... *****



%% ***** Work this detail in somewhere as a footnote. Either in the model
%% section or when discussing word2vec settings in the results *****
%% In preliminary experiments, we tested with different parameters for
%% the size of the window around the word whose usage similarity is
%% being calculated. This means we only summed the embeddings of the
%% words within this window from that word. In preliminary
%% experiments, we discovered that a window, that viewed all words in
%% the sentence, shown to produce better results in terms of
%% correlation with the gold standard.





% Big table of results and discussion
%% \begin{table*}
%% \centering
%% \small
%% \parbox{.45\linewidth}{

%% \begin{tabular}{ c | c | c }
%% W &  Dimen & Rho\\
%% \hline		

%% 1 & 50 & 0.266 \\ 
%% 1 & 100 & 0.268 \\ 
%% 1 & 300 & 0.273 \\ 
%% 2 & 50 & 0.292 \\ 
%% 2 & 100 & 0.325 \\ 
%% 2 & 300 & 0.286 \\ 
%% 5 & 50 & 0.282 \\ 
%% 5 & 100 & 0.301 \\ 
%% 5 & 300 & 0.296 \\ 
%% 8 & 50 & 0.286 \\ 
%% 8 & 100 & 0.287 \\ 
%% 8 & 300 & 0.298 \\ 
%% 10 & 50 & 0.280 \\ 
%% 10 & 100 & 0.275 \\ 
%% 10 & 300 & Training \\ 
%%   \hline 

%% \end{tabular}
%% \caption{Results on the dataset used by \cite{Gella+:2013}  using embeddings trained on content from Twitter. W = training Window
%% Dimen = dimension of the embeddings used. The Skipgram model was used}\label{tab:accents}
%% }
%% \parbox{.45\linewidth}{
%% \begin{tabular}{ c | c | c }

%% W &  Dimen & Rho\\	
%% \hline		

%% 1 & 50 & 0.221 \\ 
%% 1 & 100 & 0.248 \\ 
%% 1 & 300 & 0.262 \\ 
%% 2 & 50 & 0.229 \\ 
%% 2 & 100 & 0.259 \\ 
%% 2 & 300 & 0.278 \\ 
%% 5 & 50 & 0.259 \\ 
%% 5 & 100 & 0.269 \\ 
%% 5 & 300 & 0.279 \\ 
%% 8 & 50 & 0.285 \\ 
%% 8 & 100 & 0.275 \\ 
%% 8 & 300 & 0.280 \\ 
%% 10 & 50 & 0.275 \\
%% 10 & 100 & Training \\
%% 10 & 300 & 0.280 \\ 
%% \hline
%% \end{tabular}
%% \caption{Results on the dataset used by \cite{LuiBaldwin2012} using embeddings trained on the corpus containing content from Wikipedia. W = training Window
%% Dimen = dimension of the embeddings used. The Skipgram model was used.}\label{tab:accents}
%% }
%% \parbox{.45\linewidth}{
%% \begin{tabular}{ c | c | c }
%% W &  Dimen & Rho\\
%% \hline
%% 1 & 50 & training\\
%% 1 & 100 & training\\
%% 1 & 300 & training\\
%% 2 & 50 & 0.153 \\ 
%% 2 & 100 & 0.163 \\ 
%% 2 & 300 & 0.177 \\ 
%% 5 & 50 & 0.173 \\ 
%% 5 & 100 & 0.187 \\ 
%% 5 & 300 & 0.193 \\ 
%% 8 & 50 & 0.172 \\ 
%% 8 & 100 & 0.186 \\ 
%% 8 & 300 & 0.194 \\
%% 10 & 50 & training \\
%% 10 & 100 & training \\
%% 10 & 300 & training \\
%% \hline 
%% \end{tabular}
%% \caption{Results on the dataset used by \cite{LuiBaldwin2012} using embeddings trained on the corpus containing content from Wikipedia. W = training Window
%% Dimen = dimension of the embeddings used. The CBOW model was used.}\label{tab:accents}
%% }
%% \parbox{.45\linewidth}{
%% \begin{tabular}{ c | c | c }
%% W &  Dimen & Rho\\
%% \hline
%% 1 & 50 & Training \\
%% 1 & 100 & Training \\
%% 1 & 300 & Training \\
%% 2 & 50 & 0.126 \\ 
%% 2 & 100 & 0.169 \\ 
%% 2 & 300 & Training \\
%% 5 & 50 & Training \\
%% 5 & 100 & Training \\
%% 5 & 300 & Training \\
%% 8 & 50 & 0.165 \\ 
%% 8 & 100 & 0.189 \\ 
%% 8 & 300 & 0.200 \\
%% 10 & 50 & Training \\
%% 10 & 100 & Training \\
%% 10 & 300 & Training \\

%% \end{tabular}
%% \caption{Results on the dataset used by \cite{LuiBaldwin2012} using embeddings trained on the corpus containing content from Twitter. W = training Window
%% Dimen = dimension of the embeddings used. The CBOW model was used.}\label{tab:accents}
%% }

%% \parbox{.45\linewidth}{
%% \begin{tabular}{ c | c | c  | c}
%% w  &  D  &  CosRho  &  Regression rhou \\
%% \hline

%% 1 & 50 & 0.221 & 0.236 \\ 
%% 1 & 100 & 0.248 & 0.291 \\ 
%% 1 & 300 & 0.262 & 0.394 \\ 
%% 2 & 50 & 0.229 & 0.305 \\ 
%% 2 & 100 & 0.259 & 0.335 \\ 
%% 2 & 300 & 0.278 & 0.42 \\ 
%% 5 & 50 & 0.259 & 0.309 \\ 
%% 5 & 100 & 0.269 & 0.355 \\ 
%% 5 & 300 & 0.279 & 0.387 \\ 
%% 8 & 50 & 0.285 & 0.325 \\ 
%% 8 & 100 & 0.275 & 0.352 \\ 
%% 8 & 300 & 0.28 & 0.407 \\ 
%% 10 & 50 & 0.275 & 0.323 \\ 
%% \end{tabular}
%% \caption{Original Dataset. Rho regression with skip thought gets 0.365}\label{tab:accents}
%% }

%% \parbox{.45\linewidth}{
%% \begin{tabular}{ c | c | c  | c}
%% w  &  D  &  CosRho  &  Regression rhou \\
%% \hline

%% 1 & 50 & 0.212 & 0.137 \\ 
%% 1 & 100 & 0.275 & 0.14 \\ 
%% 1 & 300 & 0.286 & 0.044 \\ 
%% 2 & 50 & 0.236 & 0.119 \\ 
%% 2 & 100 & 0.309 & 0.139 \\ 
%% 2 & 300 & 0.319 & 0.125 \\ 
%% 5 & 50 & 0.285 & 0.157 \\ 
%% 5 & 100 & 0.322 & 0.159 \\ 
%% 5 & 300 & 0.351 & 0.148 \\ 
%% 8 & 50 & 0.304 & 0.186 \\ 
%% 8 & 100 & 0.326 & 0.207 \\ 
%% 8 & 300 & 0.364 & 0.1 \\ 
%% 10 & 50 & 0.309 & 0.196 \\ 
%% 10 & 100 & 0.322 & 0.123 \\ 
%% \end{tabular}
%% \caption{Titter Dataset.}\label{tab:accents}
%% }
%% \end{table*}



\section{Conclusions\label{sec:conclusions}}

%% Usage similarity is an approach to determining word meaning in context
%% that does not rely on a sense inventory. For text types that have a
%% relatively high rate of out-of-vocabulary words, such as social media
%% text, many words will be missing from sense inventories. Compared to
%% approaches to determining token-level word meaning such as word-sense
%% disambiguation which rely on a sense inventory, usage similarity
%% therefore seems particularly well-suited to such text types

Word senses are not discrete, and multiple senses are often applicable
for a given usage of a word. Usage similarity is an approach to
determining word meaning in context that does not rely on a sense
inventory, and therefore addresses these concerns. For text types that
have a relatively-high rate of out-of-vocabulary words, such as social
media text, many words will be missing from sense inventories; usage
similarity therefore seems particularly well-suited to such text
types, compared to approaches such as word-sense disambiguation which
rely on a sense inventory.

We presented an unsupervised approach to measuring usage similarity
based on word embeddings that achieved state-of-the-art performance on
two usage similarity datasets, one based on Twitter text, the other
based on more-conventional texts. We further considered a supervised
approach to usage similarity, and showed that although it outperforms
unsupervised approaches, it is unable to generalize to lemmas that are
unseen in the training data.

%% We presented an unsupervised approach to measuring usage similarity
%% based on word embeddings that achieved state-of-the-art performance on
%% two usage similarity datasets, based on Twitter and more-conventional
%% text, respectively. We further considered a supervised approach to
%% usage similarity, and showed that although it outperforms unsupervised
%% approaches, it is unable to generalize to lemmas that are unseen in
%% the training data.


In future work we intend to consider alternative strategies to
training supervised approaches to usage similarity in an effort to
achieve better performance on unseen lemmas. We further intend to
consider alternative ways to leverage word embeddings to represent the
meanings of tokens.


%% Future work:
%% training strategies

%% Alternative things like skip thoughts: doc2vec, DJ's new thing

%% These are really representing the meaning of a document though. We
%% intend to look at alternative ways to form vector representations
%% of tokens.

%% ***** Future work! For the Lemma case, for a given fold, we should
%% do some sort of cross-validation within the training data, to
%% determine the model parameters that best generalize to an unseen
%% lemma; possibly we should also include the cosine between the two
%% vectors as a feature? ******

%% Future work: Train a support vector regressor



%% \setlength{\baselineskip}{0.9\baselineskip}



\bibliographystyle{acl_natbib}


\let\temp\thebibliography
\renewcommand{\thebibliography}[1]{%
\temp{#1}%
\setlength{\baselineskip}{0.87\baselineskip}%
\setlength{\itemsep}{3pt}%
%% \setlength{\parsep}{1pt}%
}


\bibliography{../bibtex/big}



\end{document}
